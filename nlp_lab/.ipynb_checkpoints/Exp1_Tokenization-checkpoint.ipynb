{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e68156-bb02-4b57-a526-a39eb1b352bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'regex' has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m##from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer, TweetTokenizer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Own Function to convert sentence into words\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/nltk/__init__.py:138\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrammar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/nltk/text.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConditionalFreqDist \u001b[38;5;28;01mas\u001b[39;00m CFD\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyConcatenation, tokenwrap\n\u001b[1;32m     32\u001b[0m ConcordanceLine \u001b[38;5;241m=\u001b[39m namedtuple(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcordanceLine\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_print\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright_print\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py:65\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcasual\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TweetTokenizer, casual_tokenize\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdestructive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NLTKWordTokenizer\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegality_principle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LegalitySyllableTokenizer\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/nltk/tokenize/casual.py:215\u001b[0m\n\u001b[1;32m    208\u001b[0m REGEXPS_PHONE \u001b[38;5;241m=\u001b[39m (REGEXPS[\u001b[38;5;241m0\u001b[39m], PHONE_REGEX, \u001b[38;5;241m*\u001b[39mREGEXPS[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# TweetTokenizer.WORD_RE and TweetTokenizer.PHONE_WORD_RE represent\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# the core tokenizing regexes. They are compiled lazily.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# WORD_RE performs poorly on these patterns:\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m HANG_RE \u001b[38;5;241m=\u001b[39m \u001b[43mregex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m([^a-zA-Z0-9])\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m3,}\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# The emoticon string gets its own regex so that we can preserve case for\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# them as needed:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m EMOTICON_RE \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39mcompile(EMOTICONS, regex\u001b[38;5;241m.\u001b[39mVERBOSE \u001b[38;5;241m|\u001b[39m regex\u001b[38;5;241m.\u001b[39mI \u001b[38;5;241m|\u001b[39m regex\u001b[38;5;241m.\u001b[39mUNICODE)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'regex' has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "#Import required libraries and methods\n",
    "from typing import List \n",
    "##from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer, TweetTokenizer\n",
    "import nltk\n",
    "import re\n",
    "#Own Function to convert sentence into words\n",
    "def convert_to_words(s: str) -> List[str]:\n",
    "    ans = []\n",
    "    s += ' '\n",
    "    temp = ''\n",
    "    for i in s:\n",
    "        if i == ' ':\n",
    "            ans.append(temp)\n",
    "            temp = ''\n",
    "        else:\n",
    "            temp += i\n",
    "    return ans\n",
    "#Regex function to tokenize\n",
    "def regex(s: str) -> str:\n",
    "    word_regex_improved = r\"(\\w[\\w']*\\w|\\w)\"\n",
    "    word_matcher = re.compile(word_regex_improved)\n",
    "    return word_matcher.findall(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11b990d-bd9b-4ef4-9513-92304c5473d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a string: i am shubhi\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'convert_to_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a string: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing Own Function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconvert_to_words(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing NLTK Sentence Tokenization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msent_tokenize(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing NLTK Word Tokenization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_tokenize(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_to_words' is not defined"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter a string: \")\n",
    "print(f'Using Own Function: {convert_to_words(text)}')\n",
    "print(f'Using NLTK Sentence Tokenization: {sent_tokenize(text)}')\n",
    "print(f'Using NLTK Word Tokenization: {word_tokenize(text)}')\n",
    "print(f'Using NLTK WordPunctTokenizer: {WordPunctTokenizer().tokenize(text)}')\n",
    "print(f'Using Regex: {regex(text)}')\n",
    "print(f'Using NLTK Tweet Tokenization: {TweetTokenizer().tokenize(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4a0198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
