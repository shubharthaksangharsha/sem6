{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0172f904",
   "metadata": {},
   "source": [
    "# Experimment 2: Stemming and Lemmatization by Shubharthak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49906ebb",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "Stemming is the process of reducing word to its word stem that affixes to suffixes and prefixes or to the roots of words known as lemma. Stemming is important to NLU(Natural Language Understanding) and NLP (Natural Language Processing)\n",
    "\n",
    "## Types of Stemmer \n",
    "1. Porter Stemming \n",
    "2. Lancaster Stemming\n",
    "2. Regexp Stemming\n",
    "3. Snowball Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5091aa",
   "metadata": {},
   "source": [
    "### Using Porter Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec22b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries and methods \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68725377",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() #creating a object of the Porter Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a65a205f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using Porter Stemming:-\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->histori\n",
      "finally----->final\n",
      "finalized----->final\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\"]\n",
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Stemming using Porter Stemming:-\")\n",
    "# for word in tokenized_text:\n",
    "#     print(f'{word}----->{stemmer.stem(word)}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word}----->{stemmer.stem(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eba5da",
   "metadata": {},
   "source": [
    "### Using Lancaster Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d731676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a18bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "342ffabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using Lancaster Stemming:-\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eat\n",
      "writing----->writ\n",
      "writes----->writ\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->hist\n",
      "finally----->fin\n",
      "finalized----->fin\n"
     ]
    }
   ],
   "source": [
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Stemming using Lancaster Stemming:-\")\n",
    "# for word in tokenized_text:\n",
    "#     print(f'{word}----->{lancaster.stem(word)}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word}----->{lancaster.stem(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e725989",
   "metadata": {},
   "source": [
    "### using RegexpStemmer class \n",
    "\n",
    "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "505fdaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba3c6b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "267dbf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using Regexp Stemming:-\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->writ\n",
      "writes----->write\n",
      "programming----->programm\n",
      "programs----->program\n",
      "history----->history\n",
      "finally----->finally\n",
      "finalized----->finalized\n"
     ]
    }
   ],
   "source": [
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Stemming using Regexp Stemming:-\")\n",
    "# for word in tokenized_text:\n",
    "#     print(f'{word}----->{reg_stemmer.stem(word)}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word}----->{reg_stemmer.stem(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b142f",
   "metadata": {},
   "source": [
    "### using Snowball Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec516f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddfa95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer = SnowballStemmer('english', ignore_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ddd16d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using Snowball Stemming:-\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->histori\n",
      "finally----->final\n",
      "finalized----->final\n"
     ]
    }
   ],
   "source": [
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Stemming using Snowball Stemming:-\")\n",
    "# for word in tokenized_text:\n",
    "#     print(f'{word}----->{snowballstemmer.stem(word)}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word}----->{snowballstemmer.stem(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c000d",
   "metadata": {},
   "source": [
    "### How Snowball Stemmer is better than Porter Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6163b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fair , sport\n"
     ]
    }
   ],
   "source": [
    "print(snowballstemmer.stem('fairly'), ',',snowballstemmer.stem('sportingly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09e70c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairli , sportingli\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('fairly'), ',',stemmer.stem('sportingly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d480669",
   "metadata": {},
   "source": [
    "## Wordnet Lemmatizer\n",
    "\n",
    "Lemmatization technique is like stemming. The output we will get after lemmatization is called 'lemma', which is a root word rather than root stem. the output of stemming. After lemmatization we will be getting valid word that means the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5eeb6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lammetizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40db8b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing using WordNetLemmatizer Lammetizer:-\n",
      "eating----->eating\n",
      "eats----->eats\n",
      "eaten----->eaten\n",
      "writing----->writing\n",
      "writes----->writes\n",
      "programming----->programming\n",
      "programs----->program\n",
      "history----->history\n",
      "finally----->finally\n",
      "finalized----->finalized\n"
     ]
    }
   ],
   "source": [
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Lemmatizing using WordNetLemmatizer Lammetizer:-\")\n",
    "# for word in tokenized_text:\n",
    "#    print(f'{word}----->{lammetizer.lemmatize(word)}')\n",
    "\n",
    "for word in words:\n",
    "    print(f'{word}----->{lammetizer.lemmatize(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b3816",
   "metadata": {},
   "source": [
    "We can see not a lot of difference because of the POS is set as 'N' i.e noun but most of words are verb so we change to 'v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "317ce492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing using WordNetLemmatizer Lammetizer using Verb POS:-\n",
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eat\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->history\n",
      "finally----->finally\n",
      "finalized----->finalize\n"
     ]
    }
   ],
   "source": [
    "# text = input(\"Enter an sentence: \")\n",
    "# tokenized_text = word_tokenize(text)\n",
    "print(\"Lemmatizing using WordNetLemmatizer Lammetizer using Verb POS:-\")\n",
    "# for word in tokenized_text:\n",
    "#    print(word + '----->' + lammetizer.lemmatize(word, pos='v'))\n",
    "for word in words:\n",
    "    print(word + '----->' + lammetizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ae77d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lammetizer.lemmatize('better', pos='a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
